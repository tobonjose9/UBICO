{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9jWiaB/b/Ciw3vOs6uDuo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tobonjose9/UBICO/blob/master/Tarea2EST3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExQStwhfEihJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "file_path_test = 'sample_data/test_pruebas.csv'\n",
        "file_path_train = 'sample_data/training_pruebas.csv'\n",
        "df_test = pd.read_csv(file_path_test, low_memory=False)\n",
        "df_train = pd.read_csv(file_path_train, low_memory=False)\n",
        "\n",
        "df_nuevo = df_train['PUNT_GLOBAL']\n",
        "df_nuevo_test = pd.DataFrame(columns=['ID'])\n",
        "df_nuevo_test['ID'] = range(0, len(df_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import (mean_squared_error, mean_absolute_percentage_error,\n",
        "                           r2_score, confusion_matrix, classification_report,\n",
        "                           accuracy_score, roc_auc_score, roc_curve)"
      ],
      "metadata": {
        "id": "SHIKzGfoZhqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "pd.set_option('display.max_columns', 50)"
      ],
      "metadata": {
        "id": "U7YQL1zIZkqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('sample_data/test_pruebas.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMd0V_mtZpFc",
        "outputId": "1d1e835d-7746-4923-b6f3-c33ec1abe8fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-786bebfae5a1>:1: DtypeWarning: Columns (49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('sample_data/test_pruebas.csv')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== 1.1 Exploración de datos ===\")\n",
        "print(f\"\\nDimensiones del dataset: {df.shape}\")\n",
        "print(\"\\nResumen estadístico:\\n\", df.describe())\n",
        "print(\"\\nValores nulos por columna:\\n\", df.isnull().sum().sort_values(ascending=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "maPLdFWJfPP0",
        "outputId": "a16e98b0-5b7c-4b08-dccc-d78024adc2d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1.1 Exploración de datos ===\n",
            "\n",
            "Dimensiones del dataset: (27858, 77)\n",
            "\n",
            "Resumen estadístico:\n",
            "             PERIODO  ESTU_COD_RESIDE_DEPTO  ESTU_COD_RESIDE_MCPIO  \\\n",
            "count  27858.000000           27858.000000           27858.000000   \n",
            "mean   20231.014717             134.103920           30262.111422   \n",
            "std        0.120422            3223.923651           25914.864779   \n",
            "min    20231.000000               0.000000               4.000000   \n",
            "25%    20231.000000              11.000000           11001.000000   \n",
            "50%    20231.000000              17.000000           17001.000000   \n",
            "75%    20231.000000              52.000000           52356.000000   \n",
            "max    20232.000000           99999.000000           99999.000000   \n",
            "\n",
            "       INST_COD_INSTITUCION  ESTU_SNIES_PRGMACADEMICO  ESTU_PRGM_CODMUNICIPIO  \\\n",
            "count          27858.000000              27858.000000            27858.000000   \n",
            "mean            2511.344569              51009.264592            24672.982483   \n",
            "std             1605.205403              44902.062224            23733.028991   \n",
            "min             1101.000000                  1.000000             5001.000000   \n",
            "25%             1221.250000               4088.000000            11001.000000   \n",
            "50%             2102.000000              52356.000000            11001.000000   \n",
            "75%             2829.000000             102847.000000            41001.000000   \n",
            "max             9931.000000             111317.000000            95001.000000   \n",
            "\n",
            "       ESTU_INST_CODMUNICIPIO  ESTU_COD_MCPIO_PRESENTACION  \\\n",
            "count            27858.000000                 27858.000000   \n",
            "mean             23330.375332                 29813.336815   \n",
            "std              23051.611468                 25789.193213   \n",
            "min               5001.000000                  5001.000000   \n",
            "25%              11001.000000                 11001.000000   \n",
            "50%              11001.000000                 15238.000000   \n",
            "75%              23001.000000                 52001.000000   \n",
            "max              86001.000000                 99001.000000   \n",
            "\n",
            "       ESTU_COD_DEPTO_PRESENTACION  ESTU_INSE_INDIVIDUAL  ESTU_NSE_INDIVIDUAL  \\\n",
            "count                 27858.000000          26888.000000         26888.000000   \n",
            "mean                     29.720296             54.956615             2.666357   \n",
            "std                      25.772735              9.198395             1.109471   \n",
            "min                       5.000000             20.235238             1.000000   \n",
            "25%                      11.000000             48.649863             2.000000   \n",
            "50%                      15.000000             55.191735             2.000000   \n",
            "75%                      52.000000             61.445964             4.000000   \n",
            "max                      99.000000             87.275103             4.000000   \n",
            "\n",
            "       ESTU_NSE_IES  \n",
            "count  27858.000000  \n",
            "mean       2.519922  \n",
            "std        0.754102  \n",
            "min        2.000000  \n",
            "25%        2.000000  \n",
            "50%        2.000000  \n",
            "75%        3.000000  \n",
            "max        4.000000  \n",
            "\n",
            "Valores nulos por columna:\n",
            " ESTU_PRESENTACIONCASA          27856\n",
            "ESTU_CURSOIESEXTERNA           26278\n",
            "ESTU_ACTIVIDADREFUERZOAREAS    26278\n",
            "ESTU_SIMULACROTIPOICFES        26278\n",
            "ESTU_CURSOIESAPOYOEXTERNO      26278\n",
            "                               ...  \n",
            "ESTU_MCPIO_PRESENTACION            0\n",
            "ESTU_COD_DEPTO_PRESENTACION        0\n",
            "ESTU_DEPTO_PRESENTACION            0\n",
            "ESTU_NSE_IES                       0\n",
            "ESTU_ESTADOINVESTIGACION           0\n",
            "Length: 77, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def fix_year(date_str):\n",
        "    try:\n",
        "        return pd.to_datetime(date_str, format='%d/%m/%Y', errors='raise') # Explicitly raising errors\n",
        "    except pd.errors.OutOfBoundsDatetime: # Catching the specific pandas error\n",
        "        # Assuming the year is incorrectly formatted (e.g., 0015 instead of 2015)\n",
        "        day, month, year = date_str.split('/')\n",
        "        if len(year) == 4 and year.startswith('00'):\n",
        "            year = '20' + year[2:]  # Replace '0015' with '2015', etc.\n",
        "            return pd.to_datetime(f\"{day}/{month}/{year}\", format='%d/%m/%Y')\n",
        "        else:\n",
        "            # If the year format is unexpected, return NaT\n",
        "            return pd.NaT\n",
        "\n",
        "df['ESTU_FECHANACIMIENTO'] = df['ESTU_FECHANACIMIENTO'].apply(fix_year)\n",
        "df['EDAD'] = (datetime.now() - df['ESTU_FECHANACIMIENTO']).dt.days // 365\n",
        "\n",
        "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "cat_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Imputar numéricos con mediana\n",
        "df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
        "\n",
        "    # Imputar categóricos con moda\n",
        "for col in cat_cols:\n",
        "    df[col] = df[col].fillna(df[col].mode()[0])\n",
        "\n",
        "print(\"\\nValores nulos después de imputación:\\n\", df.isnull().sum().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecHzo8wqfjIW",
        "outputId": "e820f394-807c-4924-9efb-d11c89f11673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Valores nulos después de imputación:\n",
            " 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Codigo entero"
      ],
      "metadata": {
        "id": "Xgg_enb10IDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# CONFIGURACIÓN INICIAL (Común para todos los items)\n",
        "# ==============================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import (mean_squared_error, mean_absolute_percentage_error,\n",
        "                           r2_score, confusion_matrix, classification_report,\n",
        "                           accuracy_score, roc_auc_score, roc_curve)\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "pd.set_option('display.max_columns', 50)\n",
        "\n",
        "# Función auxiliar para preprocesamiento\n",
        "def preprocess_data(df, is_test=False):\n",
        "    if 'FECHA_NACIMIENTO' in df.columns:\n",
        "        df['FECHA_NACIMIENTO'] = pd.to_datetime(df['FECHA_NACIMIENTO'])\n",
        "        df['EDAD'] = (datetime.now() - df['FECHA_NACIMIENTO']).dt.days // 365\n",
        "\n",
        "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "    cat_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "    df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
        "\n",
        "    for col in cat_cols:\n",
        "        df[col] = df[col].fillna(df[col].mode()[0])\n",
        "        if df[col].nunique() > 10 and not is_test:\n",
        "            freq = df[col].value_counts(normalize=True)\n",
        "            df[col] = df[col].apply(lambda x: x if freq.get(x, 0) > 0.05 else 'Otros')\n",
        "\n",
        "    return df\n",
        "\n",
        "# ==============================================\n",
        "# ITEM 1: REGRESIÓN (40%)\n",
        "# ==============================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ITEM 1: REGRESIÓN (40%)\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# Cargar datos\n",
        "df = pd.read_csv('pruebas-del-saber-2025.csv')\n",
        "df = preprocess_data(df)\n",
        "\n",
        "# 1.1 Exploración de datos\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\"1.1 Exploración de datos\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "print(\"\\nDimensiones del dataset:\", df.shape)\n",
        "print(\"\\nPrimeras filas:\")\n",
        "print(df.head(3))\n",
        "print(\"\\nResumen estadístico:\")\n",
        "print(df.describe())\n",
        "print(\"\\nValores nulos por columna:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# 1.2 Análisis de correlación\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\"1.2 Análisis de correlación\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "df_dummies = pd.get_dummies(df, drop_first=True)\n",
        "corr = df_dummies.corr()['PUNT_GLOBAL'].sort_values(ascending=False)\n",
        "\n",
        "print(\"\\nVariables con mayor correlación positiva:\")\n",
        "print(corr[corr > 0.1].head(5))\n",
        "print(\"\\nVariables con mayor correlación negativa:\")\n",
        "print(corr[corr < -0.1].tail(5))\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(df_dummies.corr()[['PUNT_GLOBAL']].sort_values('PUNT_GLOBAL', ascending=False),\n",
        "            annot=True, cmap='coolwarm')\n",
        "plt.title('Correlación con PUNT_GLOBAL')\n",
        "plt.show()\n",
        "\n",
        "# 2. División y modelado\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\"2. Modelado de Regresión\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "X = df.drop(['PUNT_GLOBAL', 'FECHA_NACIMIENTO'], axis=1)\n",
        "y = df['PUNT_GLOBAL']\n",
        "\n",
        "num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "cat_cols = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())]), num_cols),\n",
        "        ('cat', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore'))]), cat_cols)])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', LinearRegression())])\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 2.3 Evaluación\n",
        "print(\"\\nEvaluación del modelo:\")\n",
        "train_pred = model.predict(X_train)\n",
        "test_pred = model.predict(X_test)\n",
        "\n",
        "print(f\"\\nR² Train: {r2_score(y_train, train_pred):.4f}\")\n",
        "print(f\"R² Test: {r2_score(y_test, test_pred):.4f}\")\n",
        "print(f\"MSE Train: {mean_squared_error(y_train, train_pred):.4f}\")\n",
        "print(f\"MSE Test: {mean_squared_error(y_test, test_pred):.4f}\")\n",
        "print(f\"MAPE Train: {mean_absolute_percentage_error(y_train, train_pred):.4f}\")\n",
        "print(f\"MAPE Test: {mean_absolute_percentage_error(y_test, test_pred):.4f}\")\n",
        "\n",
        "# 4. Kaggle Submission\n",
        "try:\n",
        "    test_kaggle = pd.read_csv('test_pruebas-del-saber-2025.csv')\n",
        "    test_kaggle = preprocess_data(test_kaggle, is_test=True)\n",
        "\n",
        "    X_kaggle = test_kaggle.drop(['FECHA_NACIMIENTO'], axis=1, errors='ignore')\n",
        "    predictions = model.predict(X_kaggle)\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        'ID': test_kaggle['ID'],\n",
        "        'PUNT_GLOBAL': predictions\n",
        "    })\n",
        "    submission.to_csv('submission_Regression.csv', index=False)\n",
        "    print(\"\\nArchivo submission_Regression.csv creado exitosamente\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError al crear submission: {str(e)}\")\n",
        "\n",
        "# 5. Hallazgos\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\"5. Principales Hallazgos\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "print(\"\\nLas variables más importantes para predecir el puntaje son:\")\n",
        "print(corr.head(5).index.tolist())\n",
        "print(\"\\nEl modelo explica aproximadamente el\",\n",
        "      round(r2_score(y_test, test_pred)*100, 2),\n",
        "      \"% de la variabilidad en los puntajes\")\n",
        "print(\"El error promedio (MAPE) es del\",\n",
        "      round(mean_absolute_percentage_error(y_test, test_pred)*100, 2),\n",
        "      \"%\")\n",
        "\n",
        "# ==============================================\n",
        "# ITEM 2: MODELO KNN (20%)\n",
        "# ==============================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ITEM 2: MODELO KNN (20%)\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# 1. Prueba con diferentes K\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\"1. Prueba con diferentes valores de K\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "results = []\n",
        "for k in [5, 10, 20, 30]:\n",
        "    model_knn = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', KNeighborsRegressor(n_neighbors=k))])\n",
        "\n",
        "    model_knn.fit(X_train, y_train)\n",
        "\n",
        "    train_pred = model_knn.predict(X_train)\n",
        "    test_pred = model_knn.predict(X_test)\n",
        "\n",
        "    results.append({\n",
        "        'vecinos': k,\n",
        "        'MSE_train': mean_squared_error(y_train, train_pred),\n",
        "        'MAPE_train': mean_absolute_percentage_error(y_train, train_pred),\n",
        "        'MSE_test': mean_squared_error(y_test, test_pred),\n",
        "        'MAPE_test': mean_absolute_percentage_error(y_test, test_pred)\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nResultados para diferentes valores de K:\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Selección del mejor K\n",
        "best_k = results_df.loc[results_df['MSE_test'].idxmin(), 'vecinos']\n",
        "print(f\"\\nMejor K según MSE de test: {best_k}\")\n",
        "\n",
        "# 2. Comparación con regresión\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\"2. Comparación con modelo de regresión\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "print(\"\\nComparación de métricas en test:\")\n",
        "print(f\"Regresión Lineal - MSE: {mean_squared_error(y_test, model.predict(X_test)):.2f}\")\n",
        "print(f\"KNN (k={best_k}) - MSE: {results_df.loc[results_df['vecinos']==best_k, 'MSE_test'].values[0]:.2f}\")\n",
        "\n",
        "if mean_squared_error(y_test, model.predict(X_test)) < results_df.loc[results_df['vecinos']==best_k, 'MSE_test'].values[0]:\n",
        "    print(\"\\nEl modelo de regresión lineal tuvo mejor desempeño en este caso\")\n",
        "else:\n",
        "    print(\"\\nEl modelo KNN tuvo mejor desempeño en este caso\")\n",
        "\n",
        "# Kaggle Submission\n",
        "try:\n",
        "    model_knn = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', KNeighborsRegressor(n_neighbors=best_k))])\n",
        "\n",
        "    model_knn.fit(X_train, y_train)\n",
        "\n",
        "    test_kaggle = pd.read_csv('test_pruebas-del-saber-2025.csv')\n",
        "    test_kaggle = preprocess_data(test_kaggle, is_test=True)\n",
        "\n",
        "    X_kaggle = test_kaggle.drop(['FECHA_NACIMIENTO'], axis=1, errors='ignore')\n",
        "    predictions = model_knn.predict(X_kaggle)\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        'ID': test_kaggle['ID'],\n",
        "        'PUNT_GLOBAL': predictions\n",
        "    })\n",
        "    submission.to_csv(f'submission_Knn_k{best_k}.csv', index=False)\n",
        "    print(f\"\\nArchivo submission_Knn_k{best_k}.csv creado exitosamente\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError al crear submission: {str(e)}\")\n",
        "\n",
        "# ==============================================\n",
        "# ITEM 3: MODELO GBM (20%)\n",
        "# ==============================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ITEM 3: MODELO GBM (20%)\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\"Entrenamiento y evaluación del modelo GBM\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "model_gbm = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', GradientBoostingRegressor(\n",
        "        n_estimators=100, learning_rate=0.1,\n",
        "        max_depth=3, random_state=42))])\n",
        "\n",
        "model_gbm.fit(X_train, y_train)\n",
        "\n",
        "# Evaluación\n",
        "train_pred = model_gbm.predict(X_train)\n",
        "test_pred = model_gbm.predict(X_test)\n",
        "\n",
        "print(\"\\nMétricas del modelo GBM:\")\n",
        "print(f\"R² Train: {r2_score(y_train, train_pred):.4f}\")\n",
        "print(f\"R² Test: {r2_score(y_test, test_pred):.4f}\")\n",
        "print(f\"MSE Train: {mean_squared_error(y_train, train_pred):.4f}\")\n",
        "print(f\"MSE Test: {mean_squared_error(y_test, test_pred):.4f}\")\n",
        "print(f\"MAPE Train: {mean_absolute_percentage_error(y_train, train_pred):.4f}\")\n",
        "print(f\"MAPE Test: {mean_absolute_percentage_error(y_test, test_pred):.4f}\")\n",
        "\n",
        "# Importancia de variables\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\"Importancia de variables en el modelo GBM\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "feature_names = (preprocessor.named_transformers_['cat']\n",
        "               .named_steps['onehot'].get_feature_names_out(cat_cols))\n",
        "all_features = list(num_cols) + list(feature_names)\n",
        "importances = model_gbm.named_steps['regressor'].feature_importances_\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': all_features,\n",
        "    'Importance': importances\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 variables más importantes:\")\n",
        "print(importance_df.head(10).to_string(index=False))\n",
        "\n",
        "# Kaggle Submission\n",
        "try:\n",
        "    test_kaggle = pd.read_csv('test_pruebas-del-saber-2025.csv')\n",
        "    test_kaggle = preprocess_data(test_kaggle, is_test=True)\n",
        "\n",
        "    X_kaggle = test_kaggle.drop(['FECHA_NACIMIENTO'], axis=1, errors='ignore')\n",
        "    predictions = model_gbm.predict(X_kaggle)\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        'ID': test_kaggle['ID'],\n",
        "        'PUNT_GLOBAL': predictions\n",
        "    })\n",
        "    submission.to_csv('submission_GBM.csv', index=False)\n",
        "    print(\"\\nArchivo submission_GBM.csv creado exitosamente\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError al crear submission: {str(e)}\")\n",
        "\n",
        "# ==============================================\n",
        "# ITEM 4: REGRESIÓN LOGÍSTICA (20%)\n",
        "# ==============================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ITEM 4: REGRESIÓN LOGÍSTICA (20%)\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# Preparación de datos\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\"Preparación de datos para regresión logística\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "df['Y'] = (df['PUNT_GLOBAL'] > 172).astype(int)\n",
        "print(\"\\nDistribución de la variable objetivo Y:\")\n",
        "print(df['Y'].value_counts(normalize=True))\n",
        "\n",
        "X = df.drop(['PUNT_GLOBAL', 'FECHA_NACIMIENTO', 'Y'], axis=1)\n",
        "y = df['Y']\n",
        "\n",
        "# División de datos\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\"Entrenamiento del modelo de regresión logística\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "model_logit = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))])\n",
        "\n",
        "model_logit.fit(X_train, y_train)\n",
        "\n",
        "# Evaluación en training\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\"Evaluación en conjunto de training\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "train_pred = model_logit.predict(X_train)\n",
        "train_proba = model_logit.predict_proba(X_train)[:, 1]\n",
        "\n",
        "print(classification_report(y_train, train_pred))\n",
        "print(f\"Accuracy: {accuracy_score(y_train, train_pred):.4f}\")\n",
        "print(f\"AUC-ROC: {roc_auc_score(y_train, train_proba):.4f}\")\n",
        "\n",
        "# Matriz de confusión training\n",
        "print(\"\\nMatriz de confusión (Training):\")\n",
        "print(confusion_matrix(y_train, train_pred))\n",
        "\n",
        "# Evaluación en validación\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\"Evaluación en conjunto de validación\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "val_pred = model_logit.predict(X_val)\n",
        "val_proba = model_logit.predict_proba(X_val)[:, 1]\n",
        "\n",
        "print(classification_report(y_val, val_pred))\n",
        "print(f\"Accuracy: {accuracy_score(y_val, val_pred):.4f}\")\n",
        "print(f\"AUC-ROC: {roc_auc_score(y_val, val_proba):.4f}\")\n",
        "\n",
        "# Matriz de confusión validación\n",
        "print(\"\\nMatriz de confusión (Validación):\")\n",
        "print(confusion_matrix(y_val, val_pred))\n",
        "\n",
        "# Variables importantes\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\"Variables más importantes en el modelo\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "try:\n",
        "    coefficients = model_logit.named_steps['classifier'].coef_[0]\n",
        "    coef_df = pd.DataFrame({\n",
        "        'Feature': all_features,\n",
        "        'Coefficient': coefficients\n",
        "    }).sort_values('Coefficient', ascending=False)\n",
        "\n",
        "    print(\"\\nTop 10 variables con mayor efecto positivo:\")\n",
        "    print(coef_df.head(10).to_string(index=False))\n",
        "    print(\"\\nTop 10 variables con mayor efecto negativo:\")\n",
        "    print(coef_df.tail(10).sort_values('Coefficient').to_string(index=False))\n",
        "except Exception as e:\n",
        "    print(f\"\\nNo se pudo obtener importancia de variables: {str(e)}\")"
      ],
      "metadata": {
        "id": "G8rEdQt9zzp6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}